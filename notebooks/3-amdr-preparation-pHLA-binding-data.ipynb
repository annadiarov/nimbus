{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset for training and training\n",
    "\n",
    "First we will generate peptide encodings in the following way:\n",
    "- Filter epitopes to 8 to 12 AA, so they can be encoded by TEIM autoencoder\n",
    "- Encode epitopes using TEIM autoencoder.\n",
    "- Scale the encoding to be between 0 and 1.\n",
    "- Save epitopes and their encoding in a dataframe in `interim` folder:\n",
    "    - `peptide`: the peptide/epitope amino acid sequence\n",
    "    - `is_mono_allelic`: whether the epitope is presented by a single HLA allele (True, False)\n",
    "    - `hla_allele`: the HLA allele or alleles that the epitope binds (name format: HLA-A-01-01)\n",
    "    - `label`: Whether the peptide binds to the HLA allele (1: Binder, 0: Non-binder)\n",
    "    - `peptide_encoding`: the encoding of the peptide. \n",
    "    - `norm_peptide_encoding`: normalized peptide encoding. \n",
    "\n",
    "Next we will process the HLA data\n",
    "- Load HLA alleles and their encoding \n"
   ],
   "id": "3bf4de9876513957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Autoencoder\n",
    "\n",
    "Load the autoencoder model and the tokenizer as shown in `2.0-amdr-exploring-peptide-encoding.ipynb`.\n",
    "\n",
    "The pretrained autoencoder will be saved as `epi_encoder` and will be used along the notebook."
   ],
   "id": "3b33ec1eb4ef2cf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:21.056301Z",
     "start_time": "2024-04-19T23:12:16.651778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CODE FROM TEIM PAPER\n",
    "#     Path on GitHub Repo:     TEIM/scripts/data_process.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, input):\n",
    "        shape = [input.shape[0]] + list(self.shape)\n",
    "        return input.view(*shape)\n",
    "    \n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "        tokenizer,\n",
    "        dim_hid,\n",
    "        len_seq,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        embedding = tokenizer.embedding_mat()\n",
    "        vocab_size, dim_emb = embedding.shape\n",
    "        self.embedding_module = nn.Embedding.from_pretrained(torch.FloatTensor(embedding), padding_idx=0, )\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(dim_emb, dim_hid, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim_hid, dim_hid, 3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.seq2vec = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(len_seq * dim_hid, dim_hid),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.vec2seq = nn.Sequential(\n",
    "            nn.Linear(dim_hid, len_seq * dim_hid),\n",
    "            nn.ReLU(),\n",
    "            View(dim_hid, len_seq)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(dim_hid, dim_hid, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(dim_hid, dim_hid, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(dim_hid),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Linear(dim_hid, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.long()\n",
    "        seq_emb = self.embedding_module(inputs)\n",
    "        \n",
    "        seq_enc = self.encoder(seq_emb.transpose(1, 2))\n",
    "        vec = self.seq2vec(seq_enc)\n",
    "        seq_repr = self.vec2seq(vec)\n",
    "        indices = None\n",
    "        seq_dec = self.decoder(seq_repr)\n",
    "        out = self.out_layer(seq_dec.transpose(1, 2))\n",
    "        return out, seq_enc, vec, indices\n",
    "\n",
    "\n",
    "def load_ae_model(tokenizer, path='./epi_ae.ckpt',):\n",
    "    # tokenizer = Tokenizer()\n",
    "    ## load model\n",
    "    model_args = dict(\n",
    "        tokenizer = tokenizer,\n",
    "        dim_hid = 32,\n",
    "        len_seq = 12,\n",
    "    )\n",
    "    model = AutoEncoder(**model_args)\n",
    "    model.eval()\n",
    "\n",
    "    ## load weights\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    state_dict = {k[6:]:v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "class PretrainedEncoder:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.ae_model = load_ae_model(tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_pretrained_epi(self, epi_seqs):\n",
    "        enc = self.infer(epi_seqs)\n",
    "        enc_vec = enc[2]\n",
    "        enc_seq = enc[-1]\n",
    "        return enc_seq, enc_vec\n",
    "    \n",
    "    def infer(self, seqs):\n",
    "        # # seqs encoding\n",
    "        n_seqs = len(seqs)\n",
    "        len_seqs = [len(seq) for seq in seqs]\n",
    "        assert (np.max(len_seqs) <= 12) and (np.min(len_seqs)>=8), ValueError('Lengths of epitopes must be within [8, 12]')\n",
    "        encoding = np.zeros([n_seqs, 12], dtype='int32')\n",
    "        for i, seq in enumerate(seqs):\n",
    "            len_seq = len_seqs[i]\n",
    "            if len_seq == 8:\n",
    "                encoding[i, 2:len_seq+2] = self.tokenizer.id_list(seq)\n",
    "            elif (len_seq == 9) or (len_seq == 10):\n",
    "                encoding[i, 1:len_seq+1] = self.tokenizer.id_list(seq)\n",
    "            else:\n",
    "                encoding[i, :len_seq] = self.tokenizer.id_list(seq)\n",
    "        # # pretrained ae features\n",
    "        inputs = torch.from_numpy(encoding)\n",
    "        out, seq_enc, vec, indices = self.ae_model(inputs)\n",
    "        out = np.argmax(out.detach().cpu().numpy(), -1)\n",
    "        return [\n",
    "            out,\n",
    "            seq_enc.detach().cpu().numpy(),\n",
    "            vec.detach().cpu().numpy(),\n",
    "            indices,\n",
    "            encoding\n",
    "        ]\n",
    "    \n",
    "# Manually load Tokenizer from their params\n",
    "tokenizer = torch.load('base_model.ckpt', map_location=torch.device('cpu'))['hyper_parameters']['model_args']['tokenizer']\n",
    "epi_encoder = PretrainedEncoder(tokenizer)"
   ],
   "id": "b4e557ae510779c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load useful functions",
   "id": "33b8599353b4d3d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:22.071706Z",
     "start_time": "2024-04-19T23:12:21.058809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "id": "bcdcfb0d5436c3f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:22.090203Z",
     "start_time": "2024-04-19T23:12:22.075709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_valid_peptide(peptide: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the peptide has valid characters\n",
    "    :param peptide: str\n",
    "        Peptide to check\n",
    "    :return: bool\n",
    "        True if peptide is valid, False otherwise\n",
    "    \"\"\"\n",
    "    peptide = peptide.upper()\n",
    "    valid_aa = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    return all([aa in valid_aa for aa in peptide])\n",
    "\n",
    "def filter_peptides_by_len(peptides_list: list,\n",
    "                           min_len: int = 8, \n",
    "                           max_len: int = 12) -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Get list of peptides that have min_len or more AA and\n",
    "    less or fewer max_len AA (min_len <= len(peptide) <= max_len)\n",
    "    :param peptides_list: list\n",
    "        List of peptides (as str) to filter\n",
    "    :param min_len: int\n",
    "        Minimum length of peptides to keep\n",
    "    :param max_len: int\n",
    "        Maximum length of peptides to keep\n",
    "    :return: Tuple[list, list]\n",
    "        * List of peptides that have min_len or more AA and\n",
    "        less or fewer max_len AA\n",
    "        * List of indices of the peptides in the original list\n",
    "        that passed the filter\n",
    "    \"\"\"\n",
    "    # Filter epitopes to 8 to 12 AA\n",
    "    filt_peptides = []\n",
    "    index_mask = []\n",
    "    n_invalid = 0\n",
    "    for i, p in enumerate(peptides_list):\n",
    "        if max_len >= len(p) >= min_len and is_valid_peptide(p):\n",
    "            filt_peptides.append(p)\n",
    "            index_mask.append(i)\n",
    "        else:\n",
    "            n_invalid +=1\n",
    "    print(f'\\t {n_invalid} peptides out of {len(seqs_epi_raw)} were not betweem {min_len} and {max_len} AA.')\n",
    "    return filt_peptides, index_mask\n",
    "\n",
    "def scale_peptide_encodings(peptide_encodings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scale each feature (col) of the peptide encodings to be between 0 and 1\n",
    "    :param peptide_encodings: np.ndarray\n",
    "        Encodings to scale\n",
    "    :return: np.ndarray\n",
    "        Scaled encodings\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(peptide_encodings)\n"
   ],
   "id": "5f32e8b2db20354",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Globals",
   "id": "13b4cb7ab4631e6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:22.114496Z",
     "start_time": "2024-04-19T23:12:22.092426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_FOLDER = os.path.join('..', 'data')\n",
    "RAW_DATA_FOLDER = os.path.join(DATA_FOLDER, 'raw')\n",
    "RAW_pHLA_BINDING_DATA_FOLDER = os.path.join(RAW_DATA_FOLDER, 'pHLA_binding')\n",
    "INTERIM_DATA_FOLDER = os.path.join(DATA_FOLDER, 'interim')\n",
    "INTERIM_pHLA_BINDING_DATA_FOLDER = os.path.join(INTERIM_DATA_FOLDER, 'pHLA_binding')\n",
    "PROCESS_DATA_FOLDER = os.path.join(DATA_FOLDER, 'processed')"
   ],
   "id": "391b28a6c875498f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process NetMHCpan data\n",
    "\n",
    "We will generate the train and test sets using the paper original split."
   ],
   "id": "f8124eccd1f5e611"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:24.843920Z",
     "start_time": "2024-04-19T23:12:24.836401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "netmhcpan_raw_train_folder = os.path.join(RAW_pHLA_BINDING_DATA_FOLDER, 'NetMHCpan_train')\n",
    "alleles_list_file = os.path.join(netmhcpan_raw_train_folder, 'allelelist')\n",
    "netmhcpan_raw_test_folder = os.path.join(RAW_pHLA_BINDING_DATA_FOLDER, 'CD8_benchmark_filtered')\n",
    "netmhcpan_interim_folder = os.path.join(INTERIM_pHLA_BINDING_DATA_FOLDER, 'NetMHCpan_dataset')"
   ],
   "id": "c88f4dbbbd8fe0b6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:29.216379Z",
     "start_time": "2024-04-19T23:12:29.191368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# binding affinity data\n",
    "ba_files = glob.glob(f'{netmhcpan_raw_train_folder}/*_ba')\n",
    "# eluted ligand data\n",
    "el_files = glob.glob(f'{netmhcpan_raw_train_folder}/*el')\n",
    "# Test set files\n",
    "test_files = glob.glob(f'{netmhcpan_raw_test_folder}/*HLA*')\n",
    "\n",
    "# Make dict with allelelist data for Multi-allelic data\n",
    "with open(alleles_list_file, 'r') as f:\n",
    "    alleles_dict = {}\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            allele, hla_list = line.split()\n",
    "            hla_list = hla_list.split(',')\n",
    "            alleles_dict[allele] = hla_list\n"
   ],
   "id": "15629e3e20995e15",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:12:39.132074Z",
     "start_time": "2024-04-19T23:12:29.758145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process Binding Affinity data\n",
    "ba_seq_pep_list = [] # List of peptide sequences for all files\n",
    "ba_epi_vec_list = [] # List of epitope encodings for all files\n",
    "ba_epi_labels_list = [] # List of epitope labels for all files\n",
    "ba_hla_list = [] # List of HLA alleles per epitope for all files\n",
    "ba_is_multi_allelic = [] # List of whether the epitope is presented by multiple alleles\n",
    "\n",
    "for f in ba_files:\n",
    "    print(f'Processing {f}')\n",
    "    df = pd.read_csv(f, sep=' ', header=None, names=('epitope', 'binding_affinity', 'hla_allele'))\n",
    "    df = df[df['hla_allele'].str.startswith('HLA')] # Only consider HLAs\n",
    "    \n",
    "    seqs_epi_raw = df['epitope'].values.tolist()\n",
    "    valid_epi, valid_epi_idx = filter_peptides_by_len(seqs_epi_raw)\n",
    "    _, epi_vec = epi_encoder.encode_pretrained_epi(valid_epi)\n",
    "    \n",
    "    df = df.iloc[valid_epi_idx] # Filter out invalid epitopes\n",
    "    binding_labels = df['binding_affinity'] >= 0.426 # Above is considered a binder\n",
    "    binding_labels_arr = binding_labels.to_numpy().astype(int)\n",
    "    \n",
    "    # Normalize HLA naming\n",
    "    hla_array = df['hla_allele'].str.replace(':', '-').values\n",
    "    \n",
    "    ba_seq_pep_list.append(df['epitope'].values)\n",
    "    ba_epi_vec_list.append(epi_vec)\n",
    "    ba_epi_labels_list.append(binding_labels_arr)\n",
    "    ba_hla_list.append(hla_array)\n",
    "    ba_is_multi_allelic.append(np.zeros(binding_labels_arr.shape[0], dtype=bool))\n",
    "    \n",
    "all_seq_pep = np.concatenate(ba_seq_pep_list)\n",
    "all_ba_epi_vec = np.concatenate(ba_epi_vec_list)\n",
    "all_ba_epi_labels = np.concatenate(ba_epi_labels_list)\n",
    "all_ba_hla = np.concatenate(ba_hla_list)\n",
    "all_ba_is_multi_allelic = np.concatenate(ba_is_multi_allelic)\n",
    "all_ba_epi_vec_norm = scale_peptide_encodings(all_ba_epi_vec)\n",
    "\n",
    "assert (all_ba_epi_vec.shape[0] == all_ba_epi_labels.shape[0] == \n",
    "        all_ba_hla.shape[0] == all_ba_is_multi_allelic.shape[0]), 'Mismatch in data shapes.'\n",
    "\n",
    "binding_affinity_df = pd.DataFrame({\n",
    "    'peptide': all_seq_pep,\n",
    "    'is_mono_allelic': ~all_ba_is_multi_allelic,\n",
    "    'hla_allele': all_ba_hla,\n",
    "    'label': all_ba_epi_labels,\n",
    "    #'peptide_encoding': [vec.tolist() for vec in all_ba_epi_vec],\n",
    "    #'norm_peptide_encoding': [vec.tolist() for vec in all_ba_epi_vec_norm]\n",
    "})\n",
    "\n",
    "binding_affinity_df.to_csv(os.path.join(netmhcpan_interim_folder, 'train_binding_affinity_peptides_data.csv.gz'), index=False)\n",
    "np.save(os.path.join(netmhcpan_interim_folder, 'train_binding_affinity_peptides_encodings.npy'), all_ba_epi_vec)\n",
    "np.save(os.path.join(netmhcpan_interim_folder, 'train_binding_affinity_peptides_encodings_norm.npy'), all_ba_epi_vec_norm)"
   ],
   "id": "df686606a7e84207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/raw/pHLA_binding/NetMHCpan_train/c003_ba\n",
      "\t 219 peptides out of 33848 were not betweem 8 and 12 AA.\n",
      "Processing ../data/raw/pHLA_binding/NetMHCpan_train/c002_ba\n",
      "\t 122 peptides out of 33974 were not betweem 8 and 12 AA.\n",
      "Processing ../data/raw/pHLA_binding/NetMHCpan_train/c000_ba\n",
      "\t 82 peptides out of 33507 were not betweem 8 and 12 AA.\n",
      "Processing ../data/raw/pHLA_binding/NetMHCpan_train/c004_ba\n",
      "\t 59 peptides out of 34613 were not betweem 8 and 12 AA.\n",
      "Processing ../data/raw/pHLA_binding/NetMHCpan_train/c001_ba\n",
      "\t 125 peptides out of 34528 were not betweem 8 and 12 AA.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32aebf32115b2649"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
