{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluate model on Binding Affinity test set\n",
    "\n",
    "Code to compute evaluate models on test set. It generates the following files\n",
    "for each evaluated model:\n",
    "- test_results.tsv: File containing labels and predictions of the model\n",
    "- test_metrics.tsv: File with global metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - Average prob of true negative labels (non-binders)\n",
    "    - Average prob of true positive labels (binders)\n",
    "    - AUROC"
   ],
   "id": "6ca14de6f7aacf8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from nimbus.predictors import pHLABindingPredictor, pHLAPseudoseqBindingPredictor\n",
    "from nimbus.data_processing import pHLADataset, pHLAPseudoseqDataset\n",
    "from nimbus.utils import LoggerFactory\n",
    "\n",
    "logger = LoggerFactory.get_logger('explore_pHLApredictor_nb', 'INFO')"
   ],
   "id": "cd85a0902c9ee252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_DIR = '../data'\n",
    "RAW_DATA = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DATA = os.path.join(DATA_DIR, 'processed')\n",
    "HLA_FP_DIR = os.path.join(PROCESSED_DATA, 'hla_fingerprints')\n",
    "hla_fp_36_data_file = os.path.join(HLA_FP_DIR, 'hla_index_netMHCpan_pseudoseq_res_representation.csv')\n",
    "hla_fp_36_file = os.path.join(HLA_FP_DIR, 'hla_fingerprint_netMHCpan_pseudoseq_res_representation.npy')\n",
    "hla_fp_400_data_file = os.path.join(HLA_FP_DIR, 'hla_af_patch_info_patch_r18_pt400.csv')\n",
    "hla_fp_400_file = os.path.join(HLA_FP_DIR, 'hla_af_patch_emb_patch_r18_pt400.npy')\n",
    "hla_pseudoseq_file = os.path.join(RAW_DATA, 'pHLA_binding', 'NetMHCpan_train', 'MHC_pseudo_fixed.dat')\n",
    "test_netmhcpan_data_file = os.path.join(PROCESSED_DATA, 'pHLA_binding', 'NetMHCpan_dataset', 'test_set_peptides_data_MaxLenPep15_hla_ABC.csv.gz')\n",
    "\n",
    "BASELINES_DIR = os.path.join('../checkpoints', 'baselines')\n",
    "NETMHCPAN_BASELINE_DIR = os.path.join(BASELINES_DIR, 'netmhcpan41')\n",
    "MIXMHCPRED22_BASELINE_DIR = os.path.join(BASELINES_DIR, 'mixmhcpred22')\n",
    "MIXMHCPRED3_BASELINE_DIR = os.path.join(BASELINES_DIR, 'mixmhcpred3')\n",
    "merged_netmhcpan_cd8_benchmark_file = os.path.join(\n",
    "    NETMHCPAN_BASELINE_DIR, 'CD8_benchmark_filtered_outs.csv.gz'\n",
    ")\n",
    "merged_mixmhcpred22_cd8_benchmark_file = os.path.join(\n",
    "    MIXMHCPRED22_BASELINE_DIR, 'CD8_benchmark_filtered_outs.csv.gz'\n",
    ")\n",
    "merged_mixmhcpred3_cd8_benchmark_file = os.path.join(\n",
    "    MIXMHCPRED3_BASELINE_DIR, 'CD8_benchmark_filtered_outs.csv.gz'\n",
    ")\n",
    "\n",
    "CHECKPOINTS_DIR = '../checkpoints/csv_logger'\n",
    "v_num = 0  # Version number\n",
    "experiments_dict = {\n",
    "    # 'pHLA_balance': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_balance', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_balance_hla_pseudoseq': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_balance_hla_pseudoseq', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 36,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    'pHLA_imbalance': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 400,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    'pHLA_imbalance_hla_pseudoseq': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    # 'pHLA_balance_FILIP128': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_balance_FILIP128', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    'pHLA_imbalance_hla_pseudoseq_ManSplits0123_4': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_ManSplits0123_4', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    'pHLA_imbalance_hla_pseudoseq_ManSplits0124_3': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_ManSplits0124_3', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    'pHLA_imbalance_hla_pseudoseq_ManSplits0134_2': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_ManSplits0134_2', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    'pHLA_imbalance_hla_pseudoseq_ManSplits0234_1': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_ManSplits0234_1', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    'pHLA_imbalance_hla_pseudoseq_ManSplits1234_0': {\n",
    "        'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_ManSplits1234_0', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "        'hla_fp_size': 36,\n",
    "        'hla_representation_type': 'surface_fp',\n",
    "    },\n",
    "    # 'pseudoseq_pHLA_imbalance_ManSplits0123_4': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pseudoseq_pHLA_imbalance_ManSplits0123_4', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 0,\n",
    "    #     'hla_representation_type': 'pseudoseq',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_newHLAFP_ManSplits0123_4': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_newHLAFP_ManSplits0123_4', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_EL_hla_pseudoseq_splitTrainTest': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_EL_hla_pseudoseq_splitTrainTest', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 36,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_EL_splitTrainTest': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_EL_splitTrainTest', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_hla_pseudoseq_AllBA_TestAsVal': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_AllBA_TestAsVal', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 36,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_hla_pseudoseq_AllEL_TestAsVal': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_hla_pseudoseq_AllEL_TestAsVal', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 36,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_newHLAFP_AllBA_TestAsVal': { # TODO test\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_newHLAFP_AllBA_TestAsVal', f'version_0', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_newHLAFP_AllBA_TestAsVal_v1': { # TODO test\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_newHLAFP_AllBA_TestAsVal', f'version_1', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pHLA_imbalance_newHLAFP_AllEL_TestAsVal': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pHLA_imbalance_newHLAFP_AllEL_TestAsVal', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 400,\n",
    "    #     'hla_representation_type': 'surface_fp',\n",
    "    # },\n",
    "    # 'pseudoseq_pHLA_imbalance_AllBA_TestAsVal': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pseudoseq_pHLA_imbalance_AllBA_TestAsVal', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 0,\n",
    "    #     'hla_representation_type': 'pseudoseq',\n",
    "    # },\n",
    "    # 'pseudoseq_pHLA_imbalance_AllEL_TestAsVal': {\n",
    "    #     'model_checkpoint': glob.glob(os.path.join(CHECKPOINTS_DIR, 'pseudoseq_pHLA_imbalance_AllEL_TestAsVal', f'version_{v_num}', 'checkpoints','ep*'))[0],\n",
    "    #     'hla_fp_size': 0,\n",
    "    #     'hla_representation_type': 'pseudoseq',\n",
    "    # },\n",
    "}"
   ],
   "id": "bd8b806884ffc198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For debugging\n",
    "test_netmhcpan_data = pd.read_csv(test_netmhcpan_data_file)\n",
    "# pick 500 random samples\n",
    "test_netmhcpan_data = test_netmhcpan_data.sample(500, replace=False, random_state=42)\n",
    "# show num 1 labels\n",
    "test_netmhcpan_data[test_netmhcpan_data['label'] == 1]"
   ],
   "id": "e3bc113684bfa299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test_netmhcpan_data = pd.read_csv(test_netmhcpan_data_file)\n",
    "hla_fp_36_emb = np.load(hla_fp_36_file)\n",
    "hla_fp_400_emb = np.load(hla_fp_400_file)\n",
    "hla_fp_36_data = pd.read_csv(hla_fp_36_data_file, index_col=1, names=['index'], header=0).to_dict()['index']\n",
    "hla_fp_400_data = pd.read_csv(hla_fp_400_data_file, index_col=1, names=['index'], header=0).to_dict()['index']\n",
    "hla_fp_36_dict = {hla: torch.Tensor(hla_fp_36_emb[idx]) for hla, idx in hla_fp_36_data.items()}\n",
    "hla_fp_400_dict = {hla: torch.Tensor(hla_fp_400_emb[idx]) for hla, idx in hla_fp_400_data.items()}\n",
    "hla_pseudoseq_dict = pd.read_csv(hla_pseudoseq_file, sep='\\s+', names=['pseudoseq'], header=None).to_dict()['pseudoseq']\n"
   ],
   "id": "5c55ac88da41e4df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset_pseudoseq_surf = pHLADataset(\n",
    "    peptide_seq_arr=test_netmhcpan_data['peptide'].values,\n",
    "    hla_names_arr=test_netmhcpan_data['hla_allele'].values, \n",
    "    hla_fp_dict=hla_fp_36_dict,\n",
    "    labels=test_netmhcpan_data['label'].values\n",
    ")"
   ],
   "id": "4fb0c041e72f5570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset_patch_surf = pHLADataset(\n",
    "    peptide_seq_arr=test_netmhcpan_data['peptide'].values,\n",
    "    hla_names_arr=test_netmhcpan_data['hla_allele'].values, \n",
    "    hla_fp_dict=hla_fp_400_dict,\n",
    "    labels=test_netmhcpan_data['label'].values\n",
    ")"
   ],
   "id": "f50605c7de5952e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset_pseudoseq = pHLAPseudoseqDataset(\n",
    "    peptide_seq_arr=test_netmhcpan_data['peptide'].values,\n",
    "    hla_names_arr=test_netmhcpan_data['hla_allele'].values,\n",
    "    hla_pseudoseq_dict=hla_pseudoseq_dict,\n",
    "    labels=test_netmhcpan_data['label'].values\n",
    ")"
   ],
   "id": "1d7f3fe016bf3504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_pretrained_model(checkpoint_file, hla_representation_type='surface_fp'):\n",
    "    if hla_representation_type == 'surface_fp':\n",
    "        logger.info(f\"Loading pHLABindingPredictor pretrained model {checkpoint_file}\")\n",
    "        model = pHLABindingPredictor.load_from_checkpoint(checkpoint_file)\n",
    "    elif hla_representation_type == 'pseudoseq':\n",
    "        logger.info(f\"Loading pHLAPseudoseqBindingPredictor pretrained model {checkpoint_file}\")\n",
    "        model = pHLAPseudoseqBindingPredictor.load_from_checkpoint(checkpoint_file)\n",
    "    else:\n",
    "        logger.error(f\"Unknown hla_representation_type {hla_representation_type}. \"\n",
    "                     f\"Expected 'surface_fp' or 'pseudoseq'\")\n",
    "        sys.exit(1)\n",
    "    return model\n"
   ],
   "id": "595afd4c3d6d3bcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#for exp_name in experiments_dict.keys():\n",
    "exp_name = 'pseudoseq_pHLA_imbalance_ManSplits0123_4'\n",
    "hla_representation_type = experiments_dict[exp_name]['hla_representation_type']\n",
    "model = load_pretrained_model(experiments_dict[exp_name]['model_checkpoint'], hla_representation_type=hla_representation_type)\n",
    "\n",
    "if hla_representation_type == 'surface_fp':\n",
    "    if 400 == experiments_dict[exp_name]['hla_fp_size']:\n",
    "        logger.info(f\"Using 400 dimensional HLA fingerprints\")\n",
    "        test_dataset = test_dataset_patch_surf\n",
    "    elif 36 == experiments_dict[exp_name]['hla_fp_size']:\n",
    "        logger.info(f\"Using 36 dimensional HLA fingerprints\")\n",
    "        test_dataset = test_dataset_pseudoseq_surf\n",
    "elif hla_representation_type == 'pseudoseq':\n",
    "    logger.info(f\"Using HLA pseudosequences\")\n",
    "    test_dataset = test_dataset_pseudoseq\n",
    "else:\n",
    "    logger.error(f\"Unknown hla_representation_type {hla_representation_type} for {exp_name}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "model.eval()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "preds = []\n",
    "labels = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    p, h, l = batch\n",
    "    reps = model(p, h) # Filip Representation\n",
    "    logits = model.linear_to_logits(reps)\n",
    "    logits = model.to_pred(logits)\n",
    "    pred = torch.sigmoid(logits).detach().cpu().numpy().tolist()\n",
    "    preds.extend(pred)\n",
    "    l = l.detach().cpu().numpy().tolist()\n",
    "    labels.extend(l)\n",
    "    \n",
    "# save results to file in checkpoint folder\n",
    "results = pd.DataFrame({'labels': labels, 'preds': preds})\n",
    "results['labels'] = results['labels'].astype(int)\n",
    "results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "results.to_csv(results_file, index=False, sep='\\t')\n",
    "logger.info(f\"Predicted test results saved to {results_file}\")\n",
    "\n",
    "metrics = {}\n",
    "# compute accuracy\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "preds_05 = preds > 0.5\n",
    "acc = np.mean(preds_05 == labels)\n",
    "metrics['accuracy'] = acc\n",
    "# compute precision: TP / (TP + FP)\n",
    "precision = precision_score(labels, preds_05)\n",
    "metrics['precision'] = precision\n",
    "# compute recall: TP / (TP + FN)\n",
    "recall = recall_score(labels, preds_05)\n",
    "metrics['recall'] = recall\n",
    "# compute auroc\n",
    "fpr, tpr, _ = roc_curve(labels, preds)\n",
    "auroc_score = auc(fpr, tpr)\n",
    "metrics['auroc'] = auroc_score\n",
    "# get pred value for negative class\n",
    "neg_prob = np.mean(preds[labels == 0])\n",
    "metrics['neg_prob'] = neg_prob\n",
    "# get pred value for positive class\n",
    "pos_prob = np.mean(preds[labels == 1])\n",
    "metrics['pos_prob'] = pos_prob\n",
    "# save metrics to file in checkpoint folder\n",
    "metrics_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_metrics.tsv')\n",
    "pd.DataFrame(metrics, index=[0]).to_csv(metrics_file, index=False, sep='\\t')\n",
    "logger.info(f\"Metrics saved to {metrics_file}\")"
   ],
   "id": "7a3177cf96a8500e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analysis on precumputed results\n",
    "## Load baselines results\n",
    "Each baseline df will have the following columns.\n",
    "- NetMHCpan4.1:\n",
    "    - MHC column with format HLA-A*02:01\n",
    "    - Score_EL column with the predicted score\n",
    "    - %Rank_EL column with the predicted rank\n",
    "    - Exp column with the true label\n",
    "    - BindLevel column with the binding level predicted with NetMHCpan4.1 default thresholds (Stron, Weak and None binders)\n",
    "- MixMHCpred2.2 and MixMHCpred3 have the same columns:\n",
    "    - MHC column with format HLA-A02-01\n",
    "    - Score column with the predicted score\n",
    "    - %Rank column with the predicted rank\n",
    "    - is_binder column with 0 or 1. Indicates if the peptide is a binder\n",
    "    - pred_is_binder column with 0 or 1. Indicates if the peptide is predicted as a binder according to the model default usage"
   ],
   "id": "9612a80ab8d99f0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load baselines data\n",
    "cd8_netmhcpan_df = pd.read_csv(merged_netmhcpan_cd8_benchmark_file)\n",
    "# change MHC colomn format from HLA-A*02:01 to HLA-A02-01\n",
    "cd8_netmhcpan_df['MHC'] = cd8_netmhcpan_df['MHC'].str.replace('*', '').str.replace(':', '-')\n",
    "\n",
    "cd8_mixmhcpred22_df = pd.read_csv(merged_mixmhcpred22_cd8_benchmark_file)\n",
    "cd8_mixmhcpred3_df = pd.read_csv(merged_mixmhcpred3_cd8_benchmark_file)\n",
    "# change mixmhcpred allele name format in BestAllele from A0201 to HLA-A02-01\n",
    "cd8_mixmhcpred22_df['BestAllele'] = 'HLA-' + cd8_mixmhcpred22_df['BestAllele'].str.slice(0,3) + '-' + cd8_mixmhcpred22_df['BestAllele'].str.slice(3, 5)\n",
    "cd8_mixmhcpred3_df['BestAllele'] = 'HLA-' + cd8_mixmhcpred3_df['BestAllele'].str.slice(0,3) + '-' + cd8_mixmhcpred3_df['BestAllele'].str.slice(3, 5)\n",
    "# rename column Score_bestAllele to Score\n",
    "cd8_mixmhcpred22_df = cd8_mixmhcpred22_df.rename(columns={'Score_bestAllele': 'Score', 'BestAllele': 'MHC', '%Rank_bestAllele': '%Rank'})\n",
    "cd8_mixmhcpred3_df = cd8_mixmhcpred3_df.rename(columns={'Score_bestAllele': 'Score', 'BestAllele': 'MHC', '%Rank_bestAllele': '%Rank'})"
   ],
   "id": "3f17afd50f4b6208",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ROC-AUC curve\n",
    "Plot ROC-AUC curve for all the models"
   ],
   "id": "fe7d616736941d62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for exp_name in experiments_dict.keys():\n",
    "    results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "    try:\n",
    "        results = pd.read_csv(results_file, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {results_file} not found\")\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(results['labels'], results['preds'])\n",
    "    auroc_score = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{exp_name} (area = {auroc_score:.2f})')\n",
    "    \n",
    "# plot baselines\n",
    "# NetMHCpan4.1\n",
    "fpr, tpr, _ = roc_curve(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['Score_EL'])\n",
    "auroc_score = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'NetMHCpan4.1_EL_score (area = {auroc_score:.2f})', linestyle='--', alpha=0.5, color='black')\n",
    "# Notice that the tpr, fpr are swapped in the plot. This is because higher values of Rank_EL indicate lower binding affinity\n",
    "tpr, fpr, _ = roc_curve(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['%Rank_EL'])\n",
    "auroc_score = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'NetMHCpan4.1_EL_rank (area = {auroc_score:.2f})', linestyle='-', alpha=0.5, color='black')\n",
    "# MixMHCpred\n",
    "# Notice that the tpr, fpr are swapped in the plot. This is because higher values of MixMHCpred indicate lower binding affinity\n",
    "tpr, fpr, _ = roc_curve(cd8_mixmhcpred22_df['is_binder'], cd8_mixmhcpred22_df['%Rank'])\n",
    "auroc_score = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'MixMHCpred2.2_Rank (area = {auroc_score:.2f})', linestyle='--', alpha=0.5, color='purple')\n",
    "tpr, fpr, _ = roc_curve(cd8_mixmhcpred3_df['is_binder'], cd8_mixmhcpred3_df['%Rank'])\n",
    "auroc_score = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'MixMHCpred3_Rank (area = {auroc_score:.2f})', linestyle='-', alpha=0.5, color='purple')\n",
    "\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='dotted', color='gray', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([-0.01, 1.01])\n",
    "# plot legend outside the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), frameon=False, facecolor='white')\n",
    "# make plot square\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n"
   ],
   "id": "ae6853e4a3071e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot Precision-Recall curve",
   "id": "2f0fd9c3add4ad56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for exp_name in experiments_dict.keys():\n",
    "    results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "    try:\n",
    "        results = pd.read_csv(results_file, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {results_file} not found\")\n",
    "        continue\n",
    "    precision, recall, _ = precision_recall_curve(results['labels'], results['preds'])\n",
    "    # compute area under the curve\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=f'{exp_name} (area = {pr_auc:.2f})')\n",
    "    \n",
    "precision, recall, _ = precision_recall_curve(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['Score_EL'])\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.plot(recall, precision, label=f'NetMHCpan4.1_EL_score (area = {pr_auc:.2f})')\n",
    "    \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlim([-0.01, 1.01])\n",
    "# plot legend outside the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), frameon=False, facecolor='white')\n",
    "# make plot square\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()\n",
    "plt.savefig('pr_curve.png')\n",
    "plt.show()"
   ],
   "id": "f57fdddfca9299bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare metrics across models",
   "id": "da97700f4a11b71b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute metrics for baselines\n",
    "\n",
    "metrics_baseline = {}\n",
    "bl_preds_05 = cd8_netmhcpan_df['Score_EL'] > 0.5\n",
    "fpr, tpr, _ = roc_curve(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['Score_EL'])\n",
    "metrics_baseline['NetMHCpan4.1_EL'] = {\n",
    "    'accuracy': np.mean(bl_preds_05 == cd8_netmhcpan_df['Exp']),\n",
    "    'precision': precision_score(cd8_netmhcpan_df['Exp'], bl_preds_05),\n",
    "    'recall': recall_score(cd8_netmhcpan_df['Exp'], bl_preds_05),\n",
    "    'auroc': auc(fpr, tpr),\n",
    "    'neg_prob': np.mean(bl_preds_05[cd8_netmhcpan_df['Exp'] == 0]),\n",
    "    'pos_prob': np.mean(bl_preds_05[cd8_netmhcpan_df['Exp'] == 1])\n",
    "}\n",
    "bl_preds_swb = cd8_netmhcpan_df['%Rank_EL'] < 2 \n",
    "metrics_baseline['NetMHCpan4.1_EL_rank'] = {\n",
    "    'accuracy': np.mean(bl_preds_swb == cd8_netmhcpan_df['Exp']),\n",
    "    'precision': precision_score(cd8_netmhcpan_df['Exp'], bl_preds_swb),\n",
    "    'recall': recall_score(cd8_netmhcpan_df['Exp'], bl_preds_swb),\n",
    "    'auroc': auc(fpr, tpr),\n",
    "    'neg_prob': np.mean(bl_preds_swb[cd8_netmhcpan_df['Exp'] == 0]),\n",
    "    'pos_prob': np.mean(bl_preds_swb[cd8_netmhcpan_df['Exp'] == 1])\n",
    "}\n",
    "\n",
    "bl_preds_rank2 = cd8_mixmhcpred22_df['%Rank'] < 2\n",
    "metrics_baseline['MixMHCpred2.2_Rank'] = {\n",
    "    'accuracy': np.mean(bl_preds_rank2 == cd8_mixmhcpred22_df['is_binder']),\n",
    "    'precision': precision_score(cd8_mixmhcpred22_df['is_binder'], bl_preds_rank2),\n",
    "    'recall': recall_score(cd8_mixmhcpred22_df['is_binder'], bl_preds_rank2),\n",
    "    'auroc': auc(fpr, tpr),\n",
    "    'neg_prob': np.mean(bl_preds_rank2[cd8_mixmhcpred22_df['is_binder'] == 0]),\n",
    "    'pos_prob': np.mean(bl_preds_rank2[cd8_mixmhcpred22_df['is_binder'] == 1])\n",
    "}\n",
    "bl_preds_rank2 = cd8_mixmhcpred3_df['%Rank'] < 2\n",
    "metrics_baseline['MixMHCpred3_Rank'] = {\n",
    "    'accuracy': np.mean(bl_preds_rank2 == cd8_mixmhcpred3_df['is_binder']),\n",
    "    'precision': precision_score(cd8_mixmhcpred3_df['is_binder'], bl_preds_rank2),\n",
    "    'recall': recall_score(cd8_mixmhcpred3_df['is_binder'], bl_preds_rank2),\n",
    "    'auroc': auc(fpr, tpr),\n",
    "    'neg_prob': np.mean(bl_preds_rank2[cd8_mixmhcpred3_df['is_binder'] == 0]),\n",
    "    'pos_prob': np.mean(bl_preds_rank2[cd8_mixmhcpred3_df['is_binder'] == 1])\n",
    "}\n",
    "\n",
    "metrics_baseline_df = pd.DataFrame.from_dict(metrics_baseline, orient='columns').T\n",
    "metrics_baseline_df"
   ],
   "id": "2202eb9f5d31250b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = {}\n",
    "for exp_name in experiments_dict.keys():\n",
    "    metrics_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_metrics.tsv')\n",
    "    try:\n",
    "        metrics[exp_name] = pd.read_csv(metrics_file, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {metrics_file} not found\")\n",
    "        continue\n",
    "# Add baseline to metrics\n",
    "for i, r in metrics_baseline_df.iterrows():\n",
    "    metrics[i] = pd.DataFrame(r).T\n",
    "    \n",
    "# Make metrics df keeping exp_name in column\n",
    "metrics_df = pd.concat(metrics).reset_index(drop=False)\n",
    "metrics_df = metrics_df.rename(columns={'level_0': 'exp_name'})\n",
    "metrics_df = metrics_df.drop(columns='level_1')\n",
    "metrics_df"
   ],
   "id": "32837b90c2c788da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot metrics\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "sns.barplot(x='exp_name', y='accuracy', data=metrics_df, ax=axs[0, 0])\n",
    "sns.barplot(x='exp_name', y='precision', data=metrics_df, ax=axs[0, 1])\n",
    "sns.barplot(x='exp_name', y='recall', data=metrics_df, ax=axs[1, 0])\n",
    "sns.barplot(x='exp_name', y='auroc', data=metrics_df, ax=axs[1, 1])\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "# rotate x labels\n",
    "for ax in axs.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha('right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4d092c6b009dfd7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix\n",
    "Make a plot with all confusion matrices. The confusion matrix is computed with a threshold of 0.5. The title is the experiment name."
   ],
   "id": "a2c46604345204da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "n_baselines = len(metrics_baseline_df)\n",
    "n_plots = len(experiments_dict.keys()) + n_baselines\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_plots / n_cols))\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 8*n_rows))\n",
    "normalize_cm = 'true' # Normalize confusion matrix to get %. Must be str among {'all', 'true', 'pred'} or None\n",
    "if normalize_cm is None:\n",
    "    fmt = 'd'\n",
    "    norm = LogNorm()\n",
    "else:\n",
    "    fmt = '.4f'\n",
    "    norm = None\n",
    "\n",
    "for i, exp_name in enumerate(experiments_dict.keys()):\n",
    "    results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "    try:\n",
    "        results = pd.read_csv(results_file, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {results_file} not found\")\n",
    "        continue\n",
    "    conf_matrix = confusion_matrix(results['labels'], results['preds'] > 0.5, normalize=normalize_cm)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap='Blues_r', ax=axs[i//n_cols, i%n_cols], norm=norm)\n",
    "    axs[i//n_cols, i%n_cols].set_title(exp_name)\n",
    "    axs[i//n_cols, i%n_cols].set_xlabel('Predicted label')\n",
    "    axs[i//n_cols, i%n_cols].set_ylabel('True label')\n",
    "    \n",
    "# Plot baselines\n",
    "conf_matrix = confusion_matrix(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['Score_EL'] > 0.5, normalize=normalize_cm)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap='Blues_r', ax=axs[(i+1)//n_cols, (i+1)%n_cols], norm=norm)\n",
    "axs[(i+1)//n_cols, (i+1)%n_cols].set_title('NetMHCpan4.1_EL score')\n",
    "axs[(i+1)//n_cols, (i+1)%n_cols].set_xlabel('Predicted label')\n",
    "axs[(i+1)//n_cols, (i+1)%n_cols].set_ylabel('True label')\n",
    "\n",
    "conf_matrix = confusion_matrix(cd8_netmhcpan_df['Exp'], cd8_netmhcpan_df['%Rank_EL'] < 2, normalize=normalize_cm)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap='Blues_r', ax=axs[(i+2)//n_cols, (i+2)%n_cols], norm=norm)\n",
    "axs[(i+2)//n_cols, (i+2)%n_cols].set_title('NetMHCpan4.1_EL rank')\n",
    "axs[(i+2)//n_cols, (i+2)%n_cols].set_xlabel('Predicted label')\n",
    "axs[(i+2)//n_cols, (i+2)%n_cols].set_ylabel('True label')\n",
    "\n",
    "conf_matrix = confusion_matrix(cd8_mixmhcpred22_df['is_binder'], cd8_mixmhcpred22_df['%Rank'] < 2, normalize=normalize_cm)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap='Blues_r', ax=axs[(i+3)//n_cols, (i+3)%n_cols], norm=norm)\n",
    "axs[(i+3)//n_cols, (i+3)%n_cols].set_title('MixMHCpred2.2_Rank')\n",
    "axs[(i+3)//n_cols, (i+3)%n_cols].set_xlabel('Predicted label')\n",
    "axs[(i+3)//n_cols, (i+3)%n_cols].set_ylabel('True label')\n",
    "\n",
    "conf_matrix = confusion_matrix(cd8_mixmhcpred3_df['is_binder'], cd8_mixmhcpred3_df['%Rank'] < 2, normalize=normalize_cm)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=fmt, cmap='Blues_r', ax=axs[(i+4)//n_cols, (i+4)%n_cols], norm=norm)\n",
    "axs[(i+4)//n_cols, (i+4)%n_cols].set_title('MixMHCpred3_Rank')\n",
    "axs[(i+4)//n_cols, (i+4)%n_cols].set_xlabel('Predicted label')\n",
    "axs[(i+4)//n_cols, (i+4)%n_cols].set_ylabel('True label')\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "419606172062d9c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Check false positives and false negatives\n",
    "\n",
    "Get information about which hla alleles are more likely to be false positives and false negatives for each model when using the threshold of 0.5."
   ],
   "id": "4803133130b3d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_netmhcpan_data = pd.read_csv(test_netmhcpan_data_file)",
   "id": "e9f542d3025c968e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_ba_data_file = os.path.join(PROCESSED_DATA, 'pHLA_binding', 'NetMHCpan_dataset', 'train_binding_affinity_peptides_data_MaxLenPep15_hla_ABC_with_BalancedSplits.csv')\n",
    "training_ba_data = pd.read_csv(training_ba_data_file)\n",
    "# count num of positive and negative labels for each hla allele\n",
    "training_ba_data['label'] = training_ba_data['label'].astype(int)\n",
    "hla_allele_counts = training_ba_data.groupby('hla_allele')['label'].value_counts().unstack().fillna(0)\n",
    "hla_allele_counts['train_total'] = hla_allele_counts.sum(axis=1)\n",
    "hla_allele_counts = hla_allele_counts.sort_values(by='train_total', ascending=False)\n",
    "hla_allele_counts = hla_allele_counts.astype(int)\n",
    "hla_allele_counts = hla_allele_counts.rename(columns={0: 'train_NB', 1: 'train_B'})\n",
    "# compute ratio of positive labels\n",
    "hla_allele_counts['train_ratio_pos_per_total'] = hla_allele_counts['train_B'] / hla_allele_counts['train_total']\n",
    "hla_allele_counts"
   ],
   "id": "60dbf48c6cc09550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for exp_name in experiments_dict.keys():\n",
    "    results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "    try:\n",
    "        results = pd.read_csv(results_file, sep='\\t')\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {results_file} not found\")\n",
    "        continue\n",
    "    results['preds_05'] = results['preds'] > 0.5\n",
    "    false_positives = results[(results['labels'] == 0) & (results['preds_05'] == 1)]\n",
    "    fp_index = false_positives.index\n",
    "    fp_data = test_netmhcpan_data.iloc[fp_index]\n",
    "    fp_hla_alleles = fp_data['hla_allele'].value_counts()\n",
    "    # add columns with training counts per allele\n",
    "    fp_hla_alleles = pd.concat([fp_hla_alleles, hla_allele_counts], axis=1, join='inner')#.sort_values(by='total', ascending=False)\n",
    "    false_negatives = results[(results['labels'] == 1) & (results['preds_05'] == 0)]\n",
    "    fn_index = false_negatives.index\n",
    "    fn_data = test_netmhcpan_data.iloc[fn_index]\n",
    "    fn_hla_alleles = fn_data['hla_allele'].value_counts()\n",
    "    # add columns with training counts per allele\n",
    "    fn_hla_alleles = pd.concat([fn_hla_alleles, hla_allele_counts], axis=1, join='inner')#.sort_values(by='total', ascending=False)\n",
    "    \n",
    "    logger.info(f\"False positives (non-binders classified as binders) for {exp_name} were {len(fp_data)}:\")\n",
    "    logger.info(fp_hla_alleles)\n",
    "    logger.info(f\"False negatives (binders classified as non-binders) for {exp_name} were {len(fn_data)}:\")\n",
    "    logger.info(fn_hla_alleles)\n",
    "    break"
   ],
   "id": "47e7958b144faabe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plot ROC-AUC curve for each hla allele\n",
    "Num of data is taken from the binding affinity train set loaded in the previous secction."
   ],
   "id": "7ba83c04ffdb9b3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_hla_alleles = test_netmhcpan_data['hla_allele'].nunique() # 52\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_hla_alleles / n_cols))\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10*n_cols, 10*n_rows))\n",
    "uniq_hla_alleles = sorted(test_netmhcpan_data['hla_allele'].unique())\n",
    "\n",
    "for i, hla_allele in enumerate(uniq_hla_alleles):\n",
    "    hla_data = test_netmhcpan_data[test_netmhcpan_data['hla_allele'] == hla_allele]\n",
    "    for exp_name in experiments_dict.keys():\n",
    "        results_file = os.path.join(os.path.dirname(experiments_dict[exp_name]['model_checkpoint']), '..', 'test_results.tsv')\n",
    "        try:\n",
    "            results = pd.read_csv(results_file, sep='\\t')\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File {results_file} not found\")\n",
    "            continue\n",
    "        hla_results = results.iloc[hla_data.index]\n",
    "        fpr, tpr, _ = roc_curve(hla_results['labels'], hla_results['preds'])\n",
    "        auroc_score = auc(fpr, tpr)\n",
    "        axs[i//n_cols, i%n_cols].plot(fpr, tpr, label=f'{exp_name} (area = {auroc_score:.2f})')\n",
    "        axs[i//n_cols, i%n_cols].set_title(f'{hla_allele} (N={hla_allele_counts.loc[hla_allele, \"train_total\"]}, r_pos={hla_allele_counts.loc[hla_allele, \"train_ratio_pos_per_total\"]:.2f})')\n",
    "        axs[i//n_cols, i%n_cols].set_xlabel('False Positive Rate')\n",
    "        axs[i//n_cols, i%n_cols].set_ylabel('True Positive Rate')\n",
    "        axs[i//n_cols, i%n_cols].set_ylim([-0.01, 1.01])\n",
    "        axs[i//n_cols, i%n_cols].set_xlim([-0.01, 1.01])\n",
    "    # Add baselines\n",
    "    fpr, tpr, _ = roc_curve(cd8_netmhcpan_df[cd8_netmhcpan_df['MHC'] == hla_allele]['Exp'], cd8_netmhcpan_df[cd8_netmhcpan_df['MHC'] == hla_allele]['Score_EL'])\n",
    "    auroc_score = auc(fpr, tpr)\n",
    "    axs[i//n_cols, i%n_cols].plot(fpr, tpr, label=f'NetMHCpan4.1_EL_score (area = {auroc_score:.2f})', linestyle='--', alpha=0.5, color='black')\n",
    "    # Notice that the tpr, fpr are swapped in the plot. This is because higher values of Rank_EL indicate lower binding affinity\n",
    "    tpr, fpr, _ = roc_curve(cd8_netmhcpan_df[cd8_netmhcpan_df['MHC'] == hla_allele]['Exp'], cd8_netmhcpan_df[cd8_netmhcpan_df['MHC'] == hla_allele]['%Rank_EL'])\n",
    "    auroc_score = auc(fpr, tpr)\n",
    "    axs[i//n_cols, i%n_cols].plot(fpr, tpr, label=f'NetMHCpan4.1_EL_rank (area = {auroc_score:.2f})', linestyle='-', alpha=0.5, color='black')\n",
    "    # MixMHCpred\n",
    "    # Notice that the tpr, fpr are swapped in the plot. This is because higher values of MixMHCpred indicate lower binding affinity\n",
    "    tpr, fpr, _ = roc_curve(cd8_mixmhcpred22_df[cd8_mixmhcpred22_df['MHC'] == hla_allele]['is_binder'], cd8_mixmhcpred22_df[cd8_mixmhcpred22_df['MHC'] == hla_allele]['%Rank'])\n",
    "    auroc_score = auc(fpr, tpr)\n",
    "    axs[i//n_cols, i%n_cols].plot(fpr, tpr, label=f'MixMHCpred2.2_Rank (area = {auroc_score:.2f})', linestyle='--', alpha=0.5, color='purple')\n",
    "    tpr, fpr, _ = roc_curve(cd8_mixmhcpred3_df[cd8_mixmhcpred3_df['MHC'] == hla_allele]['is_binder'], cd8_mixmhcpred3_df[cd8_mixmhcpred3_df['MHC'] == hla_allele]['%Rank'])\n",
    "    auroc_score = auc(fpr, tpr)\n",
    "    axs[i//n_cols, i%n_cols].plot(fpr, tpr, label=f'MixMHCpred3_Rank (area = {auroc_score:.2f})', linestyle='-', alpha=0.5, color='purple')\n",
    "    axs[i//n_cols, i%n_cols].plot([0, 1], [0, 1], lw=2, linestyle='dotted', color='gray', alpha=0.5)\n",
    "    axs[i//n_cols, i%n_cols].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), frameon=False, facecolor='white')\n",
    "    axs[i//n_cols, i%n_cols].set_aspect('equal', adjustable='box')\n",
    "    axs[i//n_cols, i%n_cols].set_ylim([-0.01, 1.01])\n",
    "    \n",
    "    # break\n",
    "    \n",
    "# Only one legend\n",
    "# handles, labels = axs[0,0].get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), frameon=False, facecolor='white')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d7c626aeef9382dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "22c1a952d3fc858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(results['labels'], results['preds']>0.5))"
   ],
   "id": "56c5896899602a54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "219dbc57b89a8440",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
